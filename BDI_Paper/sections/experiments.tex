\section{Experimental Setup}
\label{sec:experiments}

We evaluate our BDI-LLM framework on the PlanBench benchmark~\cite{valmeekam2023planbench}, a standardized evaluation suite for assessing LLM planning capabilities across classical planning domains. Our evaluation spans three domains of increasing complexity, totaling 1,270 problem instances.

\subsection{Benchmark and Domains}

\textbf{PlanBench}~\cite{valmeekam2023planbench} provides PDDL-encoded planning problems derived from the International Planning Competition (IPC). We evaluate on three domains:

\begin{itemize}
    \item \textbf{Blocksworld} (200 instances): A classic planning domain with 4 operators (\texttt{pick-up}, \texttt{put-down}, \texttt{stack}, \texttt{unstack}). Instances range from 3 to 12 objects with 2--11 goal predicates, requiring sequential manipulation of block towers while respecting physical constraints (single-arm, clear-block requirements).

    \item \textbf{Logistics} (570 instances): A transportation domain involving packages, trucks, and airplanes across multiple cities. Instances contain 7--38 objects with 1--15 delivery goals. This domain tests multi-modal transport reasoning: trucks operate within cities, airplanes fly between airports, and packages must be routed through appropriate transfer points.

    \item \textbf{Depots} (500 instances): A hybrid domain combining elements of Blocksworld and Logistics. All instances contain 18 objects with 1--3 goals. Hoists manipulate crates onto pallets or other crates, trucks transport crates between depots and distributors, requiring coordinated state tracking of hoist availability, truck positions, and crate locations.
\end{itemize}

\subsection{Model Configuration}

We use \textbf{Gemini 3 Flash Preview} (\texttt{vertex\_ai/gemini-3-flash-preview}) as the underlying LLM, accessed through the Vertex AI API. The model is configured with:
\begin{itemize}
    \item Temperature: 0.2 (low temperature for deterministic planning)
    \item Maximum output tokens: 4,000
    \item Concurrent workers: 400 (parallel instance evaluation)
\end{itemize}

\noindent Each problem instance is converted from PDDL to a structured natural language representation consisting of \emph{beliefs} (current state description with domain constraints and action specifications) and \emph{desires} (goal specification with worked examples). The LLM generates a BDI plan represented as a directed acyclic graph (DAG) of action nodes.

\subsection{Multi-Layer Verification Pipeline}

Generated plans undergo a three-layer verification pipeline:

\begin{enumerate}
    \item \textbf{Layer 1 --- Structural Verification}: Validates that the plan forms a valid DAG (no cycles, fully connected graph). An automatic repair module can insert virtual \texttt{START}/\texttt{END} nodes to unify disconnected subgraphs.

    \item \textbf{Layer 2 --- Symbolic Verification (VAL)}: Converts the BDI plan to PDDL action sequences and validates against the domain specification using the VAL plan validator~\cite{howey2004val}. This checks action preconditions, effects, and goal satisfaction. When VAL reports errors, an \emph{error-driven repair loop} re-prompts the LLM with the specific VAL error messages and cumulative repair history (up to 3 repair attempts).

    \item \textbf{Layer 3 --- Physics Validation}: For Blocksworld, a domain-specific physics simulator traces state transitions to verify physical feasibility. For Logistics and Depots, this layer is deferred to VAL (Layer 2).
\end{enumerate}

\noindent A plan is considered \emph{valid} only if it passes all applicable verification layers.

\subsection{Evaluation Metrics}

We report the following metrics:
\begin{itemize}
    \item \textbf{Overall Accuracy}: Percentage of instances where the generated plan passes all verification layers.
    \item \textbf{VAL Repair Rate}: Frequency and success rate of the error-driven repair loop.
    \item \textbf{Generation Time}: Wall-clock time from PDDL parsing to final verification.
\end{itemize}

\subsection{Baselines}

We compare against the PlanBench baseline results reported by Valmeekam et al.~\cite{valmeekam2023planbench}, where GPT-4 achieved approximately 35\% accuracy on Blocksworld and LLMs generally scored below 5\% on Logistics when evaluated without external verification or repair mechanisms. We also reference the broader findings that LLMs without structured prompting or verification typically achieve $<$30\% on multi-step planning tasks across these domains.
