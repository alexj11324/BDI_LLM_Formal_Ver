\section{Related Work}
\label{sec:related_work}

Our work draws on and contributes to three intersecting research areas: LLM-based planning, formal verification for AI-generated plans, and the BDI agent architecture. We also discuss the DSPy framework that underpins our structured prompting approach.

\subsection{LLMs for Planning}

The application of LLMs to planning tasks has attracted significant attention following demonstrations that pre-trained language models encode substantial world knowledge relevant to action sequencing.

\paragraph{Direct LLM planning.}
Early work explored using LLMs as direct planners. Huang et al.~\cite{huang2022language} showed that LLMs can decompose high-level instructions into actionable steps, while SayCan~\cite{ahn2022saycan} grounded language models in a robot's affordance functions to generate feasible action sequences. Inner Monologue~\cite{huang2023inner} extended this by incorporating environmental feedback into the LLM's planning loop, enabling re-planning after execution failures. However, these approaches lack formal correctness guarantees and rely on execution-time feedback rather than pre-execution verification.

\paragraph{Enhanced reasoning strategies.}
Several prompting techniques have been proposed to improve LLM reasoning for planning. Chain-of-Thought (CoT) prompting~\cite{wei2022chain} elicits step-by-step reasoning, while Tree of Thoughts (ToT)~\cite{yao2023tree} enables exploration of multiple reasoning paths with self-evaluation and backtracking. ReAct~\cite{yao2023react} interleaves reasoning traces with actions, allowing LLMs to query external tools during plan construction. Reflexion~\cite{shinn2023reflexion} adds a self-reflection mechanism where the LLM critiques its own outputs. While these methods improve planning performance, they operate entirely within the LLM's approximate reasoning and cannot guarantee logical correctness. Our work incorporates elements of chain-of-thought reasoning (via explicit state tracking tables) but crucially augments them with external formal verification.

\paragraph{Hybrid neuro-symbolic approaches.}
Recognizing the limitations of pure LLM planning, hybrid approaches combine LLMs with classical planners or verifiers. LLM+P~\cite{liu2023llmp} uses LLMs to translate natural language into PDDL problem specifications, then invokes a classical planner for solution. This achieves correctness but requires the LLM to produce syntactically valid PDDL, which remains challenging. The LLM-Modulo framework~\cite{kambhampati2024llms} proposes a general architecture where LLMs generate candidate plans that are checked by external ``critics'' (verifiers), with feedback loops for iterative refinement. Our work instantiates and extends this paradigm with a concrete multi-layer verification pipeline, domain-specific prompt engineering, and a structured repair mechanism that provides the LLM with precise diagnostic information rather than binary pass/fail signals.

\paragraph{Empirical evaluations.}
PlanBench~\cite{valmeekam2023planbench}, introduced at NeurIPS 2023, provides a standardized benchmark for evaluating LLM planning across classical domains. Valmeekam et al. demonstrated that GPT-4 achieves only $\sim$35\% on Blocksworld and that self-critiquing does not reliably improve plan quality~\cite{valmeekam2023can}. Follow-up work evaluated OpenAI's o1 reasoning model on PlanBench, finding improved but still imperfect performance~\cite{valmeekam2024llms}. These findings motivate our approach: rather than relying on the LLM to self-verify, we employ external formal verification tools that provide sound correctness guarantees.

\subsection{Formal Verification for AI-Generated Plans}

Formal verification has a long history in AI planning, primarily through the use of PDDL and associated tools.

\paragraph{PDDL and VAL.}
The Planning Domain Definition Language (PDDL)~\cite{mcdermott1998pddl} provides a standardized formal representation for planning domains and problems. The VAL tool~\cite{howey2004val} is the de facto standard for plan validation in the planning community, checking that action preconditions are satisfied at each step and that the goal state is achieved. Our framework leverages VAL as the core symbolic verification engine, converting LLM-generated BDI plans into PDDL action sequences for rigorous validation.

\paragraph{Verification in LLM pipelines.}
Recent work has begun integrating formal verification into LLM-based systems. Code generation pipelines use static analysis and test suites to verify LLM-generated code~\cite{chen2021codex}. In planning, Guan et al.~\cite{guan2023leveraging} use LLMs to generate PDDL domain models that are then verified against reference specifications. Silver et al.~\cite{silver2024generalized} explore using LLMs to propose planning heuristics that are formally validated. Our contribution differs in applying multi-layer verification directly to LLM-generated plans (not domain models or heuristics), with an error-driven feedback loop that enables the LLM to iteratively correct its outputs based on specific verification failures.

\subsection{BDI Agent Architecture}

The Belief-Desire-Intention (BDI) model, rooted in Bratman's theory of practical reasoning~\cite{bratman1987intention}, provides a principled framework for agent decision-making. Rao and Georgeff~\cite{rao1995bdi} formalized the BDI architecture using a branching-time temporal logic (BDICTL), establishing formal semantics for beliefs, desires, and intentions as modal operators over possible worlds.

Classical BDI implementations such as AgentSpeak~\cite{rao1996agentspeak}, Jason~\cite{bordini2007jason}, and JACK~\cite{winikoff2005jack} use hand-crafted plan libraries and logical inference for plan selection. These systems provide formal guarantees but lack the flexibility to handle natural language inputs or generate novel plans for unseen situations.

Our framework bridges this gap by using an LLM as the plan generation engine within a BDI architecture. The LLM receives beliefs (world state) and desires (goals) as structured inputs and produces intentions (plans) as structured outputs---preserving the BDI conceptual framework while leveraging the LLM's generative capabilities. The formal verification layers then provide the correctness guarantees traditionally ensured by logical inference in classical BDI systems.

\subsection{Structured LLM Programming with DSPy}

DSPy~\cite{khattab2023dspy}, introduced at ICLR 2024, provides a declarative programming framework for LLM applications. Rather than manually crafting prompt strings, developers define typed \emph{signatures} specifying input and output fields, and DSPy compiles these into optimized prompts. This approach offers several advantages for our setting: (1)~structured output parsing ensures the LLM produces well-formed BDI plan objects rather than free-text; (2)~signature docstrings serve as a principled location for domain-specific constraints and worked examples; and (3)~the framework supports systematic prompt optimization through its teleprompter modules.

Our use of DSPy extends beyond basic structured generation. We design domain-specific signatures (one per planning domain) that encode action type constraints, precondition specifications, state tracking protocols, and verified few-shot demonstrations derived from actual VAL validation traces. This represents a novel application of structured LLM programming to formal planning tasks.
