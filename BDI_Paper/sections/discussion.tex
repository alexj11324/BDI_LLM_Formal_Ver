\section{Discussion}
\label{sec:discussion}

\subsection{Why Does BDI-LLM Achieve Near-Perfect Accuracy?}

Our results demonstrate that the combination of structured BDI prompting, domain-specific natural language conversion, and multi-layer formal verification can elevate LLM planning accuracy from below 35\% to above 99\%. We attribute this to three synergistic factors:

\paragraph{Domain-Aware Prompt Engineering.}
Rather than presenting raw PDDL to the LLM, our framework converts each problem instance into a structured natural language representation that explicitly encodes: (1) the current state with typed object inventories, (2) available actions with exact parameter formats and precondition/effect specifications, (3) critical domain constraints highlighted with warning markers, (4) worked examples demonstrating correct reasoning patterns, and (5) mandatory state-tracking instructions. For Blocksworld, we additionally compute a bottom-up tower construction ordering and generate teardown steps for initial stacks that conflict with the goal, effectively reducing the planning problem to plan execution. This domain-specific conversion is key to the 100\% Blocksworld accuracy---the LLM receives not just the problem specification but a near-complete solution strategy.

\paragraph{BDI Architecture as Structured Output.}
The Belief-Desire-Intention architecture constrains the LLM's output to a well-defined DAG structure with typed action nodes and explicit dependency edges. This structured output format prevents common LLM failure modes such as generating ambiguous natural language plans, omitting action parameters, or producing plans with implicit ordering assumptions. The DAG representation also enables efficient structural verification (cycle detection, connectivity checks) before the more expensive symbolic verification.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/logistics_improvement.pdf}
\caption{Incremental improvement in Logistics domain accuracy through successive framework enhancements: domain-specific NL conversion, airport identification warnings, few-shot demonstrations, and the VAL error-driven repair loop.}
\label{fig:logistics_improvement}
\end{figure}

\paragraph{Error-Driven Repair Loop.}
The VAL-based repair loop provides a crucial safety net (Figure~\ref{fig:logistics_improvement} illustrates the cumulative impact of each framework component on Logistics accuracy). When the LLM's initial plan contains precondition violations or incorrect action sequences, the specific VAL error messages---including the failing action, the unsatisfied precondition, and a repair suggestion---are fed back to the LLM along with the cumulative history of previous repair attempts. This closed-loop feedback enables the LLM to correct localized errors without regenerating the entire plan. The high repair success rate (96.4\% overall, 98.7\% for Logistics) suggests that most initial errors are minor state-tracking mistakes rather than fundamental reasoning failures.

\subsection{Analysis of Failure Modes}

The 5 failed instances (0.4\%) reveal two categories of LLM limitations:

\paragraph{Graph Structure Errors.}
Instance-166 (Logistics) produced a cyclic plan graph, indicating that the LLM failed to maintain a consistent temporal ordering across 15 interdependent actions involving 3 cities, 3 airplanes, and 5 packages. Cyclic dependencies represent a fundamental violation of plan semantics that cannot be addressed by the VAL repair loop, as the plan structure itself is invalid. This failure mode is rare (1/1,270 = 0.08\%) but highlights the challenge of maintaining global consistency in complex multi-agent coordination scenarios.

\paragraph{Persistent State Tracking Failures.}
The remaining 4 failures involve the LLM losing track of object positions after movement actions. In the Depots domain, two instances failed because the model attempted to load crates onto trucks at locations the trucks had already departed from. Despite receiving explicit VAL error messages identifying the unsatisfied precondition (\texttt{at truck2 depot2}), the repair attempts failed to correct the truck position tracking. This suggests that certain state-tracking errors are deeply embedded in the LLM's reasoning chain and cannot be resolved through local repairs alone.

\subsection{The Role of VAL in the Verification Pipeline}

Our results underscore the importance of formal verification in LLM-based planning. Without the VAL verification layer, plans that appear structurally valid (correct DAG structure) but contain semantic errors (precondition violations) would be accepted as correct. In our experiments, 111 instances (8.7\%) required VAL-based repair, meaning that a system relying solely on structural verification would have achieved approximately 91.2\% accuracy instead of 99.6\%. The VAL validator serves as both a correctness guarantee and an error signal generator for the repair loop.

\subsection{Limitations}

\paragraph{Domain-Specific Engineering.}
The current framework requires domain-specific natural language conversion functions for each planning domain. While the conversion follows a consistent pattern (state description, action specification, constraint enumeration), extending to new domains requires manual engineering of the PDDL-to-NL conversion and domain-specific prompt elements. Future work could explore automated domain adaptation using the PDDL domain specification itself.

\paragraph{Scalability.}
Our evaluation covers instances with up to 38 objects and 15 goals. Performance on significantly larger instances (e.g., 100+ objects) remains untested. The LLM's context window and reasoning capacity may degrade for very large problem instances, particularly in domains requiring long action sequences.

\paragraph{Single LLM Dependency.}
All experiments use Gemini 3 Flash Preview. While this demonstrates the framework's effectiveness with a specific model, generalization to other LLMs (GPT-4, Claude, Llama) requires further evaluation. The framework's architecture is model-agnostic, but the optimal prompt engineering may vary across models.

\paragraph{Repair Loop Ceiling.}
The repair loop is limited to 3 attempts, and 4 of the 5 failures involved repair attempts that did not succeed. Increasing the repair budget or employing alternative repair strategies (e.g., plan-from-scratch regeneration, decomposition into subproblems) could potentially reduce the failure rate further.

\subsection{Comparison with Other Approaches}

Our work differs from prior LLM planning approaches in several key aspects:

\begin{itemize}
    \item \textbf{LLM+P}~\cite{liu2023llmp}: Translates natural language to PDDL and uses classical planners. Our approach keeps the LLM as the planner but adds formal verification, avoiding the brittleness of LLM-generated PDDL.

    \item \textbf{Tree-of-Thought}~\cite{yao2023tree}: Uses search-based prompting for multi-step reasoning. Our BDI framework provides a more structured output format and leverages domain-specific verification rather than generic search.

    \item \textbf{ReAct}~\cite{yao2023react}: Interleaves reasoning and acting. Our approach generates complete plans upfront and verifies them post-hoc, enabling formal correctness guarantees rather than runtime error recovery.

    \item \textbf{SayCanPay}~\cite{hazra2024saycanpay}: Combines LLM planning with cost-based action selection. Our framework focuses on correctness verification rather than cost optimization, achieving higher accuracy through formal methods.
\end{itemize}
